{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f941f9",
   "metadata": {},
   "source": [
    "```python\n",
    "Math 579 Project Documentation\n",
    "\n",
    "Date: 2025-04-21  \n",
    "Name: Samisoni Palu  \n",
    "Instructor: Dr. Sun\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7c137",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.1' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chess\n",
    "from chess import pgn\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('c:\\\\Users\\\\samip\\\\Documents\\\\quick-maffs\\\\neural_nets\\\\makemorechessmoves')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849aa8e6",
   "metadata": {},
   "source": [
    "```pyton\n",
    "## Utils\n",
    "Board data\n",
    "---\n",
    "`generate_uci_move_list`\n",
    "- list all possible moves as a 2-tuple of from_square, to_square\n",
    "- tweaked for promotion move counts\n",
    "- will Include Redundant or Invalid Moves\n",
    "    - **`a1a1`, `d4d4`**, etc. → no actual move is made\n",
    "    - **Illegal under any real game condition**\n",
    "    - Also includes nonsense like `h2h8` (rook-style moves for pawns)\n",
    "\n",
    "But remember:  \n",
    "> You’re not saying “these are all valid moves”  \n",
    "> **this is the full move *vocabulary***—the possible *labels* in a classification task.\n",
    "> That is, our **vocabulary** list is all the two-tuple pairings of squares, e.g. (`h2g3`), along with other special moves\n",
    "\n",
    "Our total vocabulary size count includes the sum of\n",
    "- 64x64 (each pairing of squares)\n",
    "- 8 (number of columns) x 2 (white-black promotions) x 4 (choices of upgrade) x 3 (capture types) - 16 (edge cases)\n",
    "\n",
    "We should expect our logits vector to have size **4272**. \n",
    "\n",
    "**Benefits of the chosen Vocabulary**\n",
    "\n",
    "1. Simplicity in Output Shape\n",
    "    - 1-to-1 mapping: index ↔ UCI\n",
    "    - You don’t need dynamic output heads or custom decoders\n",
    "    - You can store logits as `torch.tensor([4672])` and just mask out illegal ones at runtime\n",
    "\n",
    "2. Consistency\n",
    "    - Your label space is fixed across:\n",
    "      - Training\n",
    "      - Inference\n",
    "      - Evaluation\n",
    "\n",
    "3. Non-moves Never Get Trained On\n",
    "    - No master ever plays `a1a1`\n",
    "    - So those output indices **never get gradient updates**\n",
    "    - They just sit in the model—harmless dead neurons\n",
    "\n",
    "**Why You Might Remove Redundant Moves**\n",
    "\n",
    "1. Smaller Output Space\n",
    "    - Saves compute on final linear layer and softmax\n",
    "    - Slightly faster training (maybe)\n",
    "\n",
    "2. Model Capacity Allocation\n",
    "    - You force the network to **only ever consider valid move templates**\n",
    "    - Could lead to sharper learning curve\n",
    "\n",
    "But you pay with **more complexity**:\n",
    "- Dynamic move indexing\n",
    "- Pre-mask needs to align with training mask\n",
    "- Harder debugging\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb991bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uci_move_list():\n",
    "    all_moves = set()\n",
    "    for from_sq in chess.SQUARES:\n",
    "        for to_sq in chess.SQUARES:\n",
    "            move = chess.Move(from_sq, to_sq)\n",
    "            all_moves.add(move.uci())\n",
    "            # Add promotions\n",
    "            for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                from_rank = chess.square_rank(from_sq)\n",
    "                to_rank = chess.square_rank(to_sq)\n",
    "                from_file = chess.square_file(from_sq)\n",
    "                to_file = chess.square_file(to_sq)\n",
    "                # Only allow forward promotion (white or black)\n",
    "                if (from_rank , to_rank) in [(6, 7), (1, 0)]:  # white/black promotion ranks\n",
    "                    if abs(from_file - to_file) <= 1:         # straight or diagonal\n",
    "                        promo_move = chess.Move(from_sq, to_sq, promotion=promo)\n",
    "                        all_moves.add(promo_move.uci())\n",
    "    return sorted(all_moves)\n",
    "\n",
    "def save_move_index_map(path=\"data/move_index_map.json\"):\n",
    "    moves = generate_uci_move_list()\n",
    "    uci_to_index = {uci: i for i, uci in enumerate(moves)}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(uci_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_move_index_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_list = generate_uci_move_list()\n",
    "print('Logits size = ',len(move_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# board state gets entries from cuhh\n",
    "PIECE_TO_IDX = {\n",
    "    None: 0,\n",
    "    chess.PAWN: 1,\n",
    "    chess.KNIGHT: 2,\n",
    "    chess.BISHOP: 3,\n",
    "    chess.ROOK: 4,\n",
    "    chess.QUEEN: 5,\n",
    "    chess.KING: 6,\n",
    "}\n",
    "\n",
    "# cuhh make the board go mathematical\n",
    "def encode_board(board: chess.Board):\n",
    "    board_array = np.zeros((8, 8), dtype=np.int64)\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        row = 7 - (square // 8)\n",
    "        col = square % 8\n",
    "\n",
    "        if piece is not None:\n",
    "            base = PIECE_TO_IDX[piece.piece_type]\n",
    "            offset = 0 if piece.color == chess.WHITE else 6\n",
    "            board_array[row][col] = base + offset\n",
    "        else:\n",
    "            board_array[row][col] = 0  # empty\n",
    "\n",
    "    return board_array  # shape: [8,8] of ints in [0,12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it go right here\n",
    "board = chess.Board()\n",
    "encode_board(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just a mlp frfr\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32, num_moves=4272):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(13, embedding_dim)  # 13 tokens -> vector\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                # [8,8,32] -> [2048]\n",
    "            nn.Linear(8*8*embedding_dim, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, num_moves)   # Final logits\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # x: [B, 8, 8]\n",
    "        x = self.embed(x)  # [B, 8, 8, D]\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73285571",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PolicyNet(embedding_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d805",
   "metadata": {},
   "source": [
    "### What this do frfr ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479fa28",
   "metadata": {},
   "source": [
    "We want a neural network that takes:\n",
    "- Input: an `8×8` grid of piece tokens (entries are integers from 0 to 12)\n",
    "- Output: a **4272-dimensional logits vector**\n",
    "\n",
    "---\n",
    "\n",
    "### CLASS STRUCTURE: `PolicyNet`\n",
    "\n",
    "```python\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32, num_moves=4272):\n",
    "```\n",
    "\n",
    "- **`embedding_dim=32`**: each board square will be represented by a **32-dimensional vector**.\n",
    "- **`num_moves=4272`**: the size of **output layer**, corresponding to all possible moves.\n",
    "\n",
    "--- \n",
    "\n",
    "```python\n",
    "        super().__init__()\n",
    "```\n",
    "\n",
    "- Standard for initializing the parent class (`nn.Module`).\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "```python\n",
    "        self.embed = nn.Embedding(13, embedding_dim)\n",
    "```\n",
    "\n",
    "- This layer turns each square’s integer (0–12) into a vector of dimension `[embedding_dim]`.\n",
    "- `[8,8]` board → `[8,8,32]` tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Connected Network (MLP)\n",
    "These are our hidden and output logits layer. \n",
    "```python\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                \n",
    "            nn.Linear(8*8*embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_moves)\n",
    "        )\n",
    "```\n",
    "\n",
    "#### `nn.Flatten()`\n",
    "- Converts `[B, 8, 8, 32]` into `[B, 2048]`, similar to .view\n",
    "- Needed to feed into `Linear` layers\n",
    "\n",
    "#### `nn.Linear(2048, 512)`\n",
    "- Fully connected layer reducing 2048 features to 512 neurons\n",
    "\n",
    "#### `nn.ReLU()`\n",
    "- Non-linear activation to let the model learn more complex patterns\n",
    "\n",
    "#### `nn.Linear(512, num_moves)`\n",
    "- Final layer: predicts logit value for each of the 4672 moves\n",
    "\n",
    "---\n",
    "\n",
    "### `forward` Function\n",
    "\n",
    "```python\n",
    "    def forward(self, x):  # x: [B, 8, 8]\n",
    "        x = self.embed(x)  # [B, 8, 8, D]\n",
    "        x = x.permute(0, 3, 1, 2)  # Optional: [B, D, 8, 8]\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "- **Input**: `x` is a batch of boards, shape `[batch_size, 8, 8]`\n",
    "- **Embedding**: turns each square into a vector: `[B, 8, 8, 32]`\n",
    "- **FC Network**: outputs a `[B, 4272]` tensor of logits\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- network **understands piece identity** through embeddings.\n",
    "- it **flattens** the board to make a prediction using fully connected layers.\n",
    "- The output is a **score for every possible move**, and later **mask illegal ones**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PolicyNet(embedding_dim=2)\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028e809",
   "metadata": {},
   "source": [
    "## DATA SETS\n",
    "\n",
    "Documentation *X*:**INPUT** and *Y*:**TARGET** \n",
    "\n",
    "*X* is a **board state**.\n",
    "\n",
    "> **Board state:**  \n",
    "> - *8×8 matrix*  \n",
    "> - Entries in `[0, 12]`  \n",
    "> - Entries map to **piece type**  \n",
    "\n",
    "*Y* is a list of **move indices**.\n",
    "\n",
    ">**Move Indices (`logits`):**\n",
    "> - vector in $\\mathbb{R}^{4272}$\n",
    "> - entries map to a **move**\n",
    "\n",
    "A reminder of how moves are defined follows. \n",
    "\n",
    ">**Move:**\n",
    "> - pair of grid values (Why?)\n",
    "> - e.g. *d3d2*\n",
    "\n",
    "Recall that the policy net (MLP) outputs a **softmaxed** list of (logits). Training occurs via `cross_entropy`. We read move data from text file in **PGN form**. X lists all the board states of a given game in the order it was played. Y lists all the moves made from a given board state. Let $S_i$ be the $i$ -th board state of the given game, and let $a_i$ be the action (move) made from this state. Then, we may visualize $X$ and $Y$ as the following:\n",
    "\n",
    "$$\\text{Input:}\\ S_1,\\ S_2,\\ ...,\\ S_n$$\n",
    "\n",
    "$$\\text{Output:}\\ a_1,\\ a_2,\\ ...,\\ a_n$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cef91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = 'C:/Users/samip/Documents/quick-maffs/neural_nets/makemorechessmoves/data/raw_games/Carlsen.pgn'\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_move_index_map(path=Path(\"data/move_index_map.json\")):\n",
    "    path = Path(path)\n",
    "    assert path.exists(), f\"Move index map not found at {path}\"\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "def load_dataset(raw, move_map_p ath = \"data/move_index_map.json\"):\n",
    "    move_index_map = load_move_index_map(move_map_path)\n",
    "\n",
    "    with open(raw, \"r\") as file:\n",
    "        game = pgn.read_game(file)\n",
    "        board = game.board()\n",
    "        X, Y = [], []\n",
    "\n",
    "        for move in game.mainline_moves():\n",
    "           board_state = encode_board(board)  # BEFORE the move\n",
    "           try:\n",
    "               move_index_map = load_move_index_map(move_map_path)\n",
    "               move_index = move_index_map[move.uci()] # move is INDEXED here, move list starts from first move\n",
    "               X.append(board_state) # first board state is neutral board, it must be appended to have good beginning game\n",
    "               Y.append(move_index)\n",
    "               board.push(move)  # Move AFTER data capture\n",
    "           except ValueError:\n",
    "               print(f\"Move {move.uci()} not found in move_list. Skipping this move.\")\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "#what happens if we remove winning move (movelist[-1]), will the model still choose victory? >:P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'key1':1,'key2':2}\n",
    "dict['key1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_dataset(in_data)\n",
    "n1 = int(0.8*len(X))\n",
    "n2 = int(0.9*len(X))\n",
    "\n",
    "# shuffled indices\n",
    "newInd = torch.randint(low=0,high=Y.shape[0],size = (X.shape[0],))\n",
    "\n",
    "# shuffle X and Y to match\n",
    "X = X[newInd]\n",
    "Y = Y[newInd]\n",
    "\n",
    "Xtr, Ytr = X[:n2],Y[:n2]\n",
    "Xdev, Ydev = X[n2:],Y[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82647cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from the first linear layer\n",
    "weights = mlp.fc[1].weight.detach().numpy()\n",
    "\n",
    "# Plot the weights as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(weights, aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Weights of the First Linear Layer\")\n",
    "plt.xlabel(\"Input Features\")\n",
    "plt.ylabel(\"Output Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024646d9",
   "metadata": {},
   "source": [
    "**A Note on Initialization**\n",
    "\n",
    "> We must initialize our layers carefully in order to optimize training speed in the gradient step. \n",
    "\n",
    "Consider $tanh$ activation for instance: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10,10,100), np.tanh(np.linspace(-10,10,100)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcc652",
   "metadata": {},
   "source": [
    "Note that gradient values for large $x$ of `tanh` are close to 0. In essence, neurons may be killed off too early with a sloppy initialization. \n",
    "\n",
    "Due to this fact, you might consider `ReLu` for its beneficial properties, including: \n",
    "\n",
    "- no upper bound\n",
    "- no vanishing for positive inputs\n",
    "- easier initialization\n",
    "\n",
    "allowing for faster training and increased stability. Our policy net (MLP) already initializes efficiently using layers defined by the `nn` package, but the point is clear, \n",
    "\n",
    "> We should fix our initializations depending on what activation function is used. \n",
    "\n",
    "This is useful later when we compare different bot models in competition. \n",
    "\n",
    "**Training Step**\n",
    "\n",
    "We start with a very minimal training loop.\n",
    "- static learning rate\n",
    "- no optimizers\n",
    "- no batch normalizations (in fact, we won't need it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee478fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PolicyNet(embedding_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_step = 10\n",
    "count_step_tops = 10\n",
    "lossi = []\n",
    "num_steps = 1000\n",
    "#lr = torch.logspace(-3, 0, steps=num_steps)\n",
    "lr = 0.23\n",
    "batch_size = 16\n",
    "top1_list, top5_list = [], []\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch = torch.randint(low=0,high=Ytr.shape[0], size=(batch_size,))\n",
    "\n",
    "    logits = mlp(Xtr[batch])\n",
    "    loss = F.cross_entropy(logits, Ytr[batch])\n",
    "\n",
    "    \n",
    "    for p in mlp.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters():\n",
    "        p.data += -lr*p.grad\n",
    "    if i%count_step==0:\n",
    "        print(f'Step[{i}]: Current loss value = {loss}')\n",
    "    if i%count_step_tops == 0: \n",
    "        top5 = torch.topk(logits, k=5, dim=1).indices  # [batch, k]\n",
    "        correct = top5.eq(Ytr[batch].view(-1, 1)).any(dim=1).float()  # 1 if Y in top-k\n",
    "        top5_acc = correct.mean()\n",
    "\n",
    "        top1 = torch.topk(logits, k=1, dim=1).indices  # [batch, k]\n",
    "        correct = top1.eq(Ytr[batch].view(-1, 1)).any(dim=1).float()  # 1 if Y in top-k\n",
    "        top1_acc = correct.mean()\n",
    "\n",
    "        top1_list.append(top1_acc)\n",
    "        top5_list.append(top5_acc)\n",
    "        print(f'Top-1: {top1_acc}, Top-5: {top5_acc}')\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Loss\n",
    "axs[0].plot(lossi, color='red')\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[0].set_xlabel(\"Step Number\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "# Plot Top-1 Accuracy\n",
    "axs[1].plot(top1_list, color='blue')\n",
    "axs[1].set_title(\"Top-1 Accuracy\")\n",
    "axs[1].set_xlabel(\"Step Number (Log Scale)\")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Plot Top-5 Accuracy\n",
    "axs[2].plot(top5_list, color='green')\n",
    "axs[2].set_title(\"Top-5 Accuracy\")\n",
    "axs[2].set_xlabel(\"Step Number\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "\n",
    "axs[2].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    val_logits = mlp(Xdev)\n",
    "    val_loss = F.cross_entropy(val_logits, Ydev)\n",
    "    print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PolicyNet(embedding_dim=2)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9739ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "count_step = 10\n",
    "count_step_tops = 10\n",
    "lossi = []\n",
    "num_steps = 1000\n",
    "#lr = torch.logspace(-3, 0, steps=num_steps)\n",
    "lr = 0.002\n",
    "batch_size = 16\n",
    "top1_list, top5_list = [], []\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "\n",
    "grad_log = []\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch = torch.randint(low=0,high=Ytr.shape[0], size=(batch_size,))\n",
    "    optimizer.zero_grad()\n",
    "    logits = mlp(Xtr[batch])\n",
    "    loss = F.cross_entropy(logits, Ytr[batch])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    if i%count_step==0:\n",
    "        print(f'Step[{i}]: Current loss value = {loss}')\n",
    "    if i%count_step_tops == 0: \n",
    "        top5 = torch.topk(logits, k=5, dim=1).indices  # [batch, k]\n",
    "        correct = top5.eq(Ytr[batch].view(-1, 1)).any(dim=1).float()  # 1 if Y in top-k\n",
    "        top5_acc = correct.mean()\n",
    "\n",
    "        top1 = torch.topk(logits, k=1, dim=1).indices  # [batch, k]\n",
    "        correct = top1.eq(Ytr[batch].view(-1, 1)).any(dim=1).float()  # 1 if Y in top-k\n",
    "        top1_acc = correct.mean()\n",
    "\n",
    "        top1_list.append(top1_acc)\n",
    "        top5_list.append(top5_acc)\n",
    "        print(f'Top-1: {top1_acc}, Top-5: {top5_acc}')\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Loss\n",
    "axs[0].plot(lossi, color='red')\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[0].set_xlabel(\"Step Number\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "# Plot Top-1 Accuracy\n",
    "axs[1].plot(top1_list, color='blue')\n",
    "axs[1].set_title(\"Top-1 Accuracy\")\n",
    "axs[1].set_xlabel(\"Step Number (Log Scale)\")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Plot Top-5 Accuracy\n",
    "axs[2].plot(top5_list, color='green')\n",
    "axs[2].set_title(\"Top-5 Accuracy\")\n",
    "axs[2].set_xlabel(\"Step Number\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "\n",
    "axs[2].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get weights of a layer\n",
    "weights = mlp.fc[1].weight.detach().cpu().numpy()  # Assuming layer 1 is Linear\n",
    "\n",
    "# Flatten and plot\n",
    "plt.hist(weights.flatten(), bins=100)\n",
    "plt.title(\"Weight Distribution (Before Tanh)\")\n",
    "plt.xlabel(\"Weight value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in mlp.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grads = param.grad.detach().cpu().flatten().numpy()\n",
    "        plt.hist(grads, bins=100)\n",
    "        plt.title(f\"Gradient Distribution: {name}\")\n",
    "        plt.xlabel(\"Gradient value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6c6ca",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "### Independents\n",
    "**Optimizers**\n",
    "**LRs**\n",
    "**Activation Functions**\n",
    "**vs Stockfish**\n",
    "**Model Dimensions**\n",
    "- layer no\n",
    "- layer size\n",
    "**embed vs one hot**\n",
    "**embedding dimensions**\n",
    "**batch size in training**\n",
    "**argmax vs softmax**\n",
    "**different training data**\n",
    "\n",
    "\n",
    "### Metrics\n",
    "**Win Rate**\n",
    "**Training Speed**\n",
    "**Stability**\n",
    "**embedding visualization**\n",
    "\n",
    "### To do \n",
    "- embedding visualization\n",
    "\n",
    "### Functionality\n",
    "\n",
    "With our first test, we should gauge how well the base model plays. We first define a move selecting function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae24851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_move_softmax(model, board, move_index_to_uci, uci_to_index, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the board\n",
    "        board_tensor = encode_board(board)\n",
    "        board_tensor = torch.tensor(board_tensor, dtype=torch.long).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(board_tensor)[0]  # remove batch dim, shape [4272]\n",
    "        \n",
    "        # Mask illegal moves\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        legal_move_indices = []\n",
    "        for move in legal_moves:\n",
    "            uci = move.uci()\n",
    "            if uci in uci_to_index:\n",
    "                legal_move_indices.append(uci_to_index[uci])\n",
    "\n",
    "        mask = torch.zeros_like(logits)\n",
    "        mask[legal_move_indices] = 1\n",
    "\n",
    "        # mask\n",
    "        logits = logits + (mask - 1) * 1e9  # large negative for illegal moves\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Softmax\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        # Sample\n",
    "        move_index = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        move_uci = move_index_to_uci[move_index]\n",
    "        return chess.Move.from_uci(move_uci)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_uci = {index:move for index,move in enumerate(move_list)}\n",
    "uci_to_index = {move:index for index,move in enumerate(move_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288c93d",
   "metadata": {},
   "source": [
    "... as well as a game loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99877b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vs_itself(): \n",
    "    game = []\n",
    "    board = chess.Board()\n",
    "    while not board.is_game_over():\n",
    "        move = select_move_softmax(mlp, board, index_to_uci, uci_to_index, temperature=1.0)\n",
    "        board.push(move)\n",
    "        game.append(move.uci())\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three games\n",
    "game1 = model_vs_itself()\n",
    "game2 = model_vs_itself()\n",
    "game3 = model_vs_itself()\n",
    "\n",
    "print(len(game1))\n",
    "print(len(game2))\n",
    "print(len(game3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0556ec",
   "metadata": {},
   "source": [
    "Typically, well-played games range between 40-80 moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_game = chess.pgn.Game()\n",
    "pgn = pgn_game\n",
    "\n",
    "\n",
    "for move in game:\n",
    "    pgn = pgn.add_variation(chess.Move.from_uci(move))\n",
    "\n",
    "print(pgn_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_game.pgn\", \"w\") as file:\n",
    "    file.write(str(pgn_game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 1, 500)  # Avoid 0 to prevent issues with log scale\n",
    "y = x**3\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=\"$x^3$\")\n",
    "plt.plot(x,x**2, label=\"$x^2$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Plot of $x^3$ on [0, 1] with Logarithmic Scale\")\n",
    "plt.xlabel(\"x (log scale)\")\n",
    "plt.ylabel(\"y (log scale)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9955ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
