{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f941f9",
   "metadata": {},
   "source": [
    "```python\n",
    "Math 579 Project Documentation\n",
    "\n",
    "Date: 2025-04-21  \n",
    "Name: Samisoni Palu  \n",
    "Instructor: Dr. Sun\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51b7c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849aa8e6",
   "metadata": {},
   "source": [
    "```pyton\n",
    "## Utils\n",
    "Board data\n",
    "---\n",
    "`generate_uci_move_list`\n",
    "- list all possible moves as a 2-tuple of from_square, to_square\n",
    "- tweaked for promotion move counts\n",
    "- will Include Redundant or Invalid Moves\n",
    "    - **`a1a1`, `d4d4`**, etc. → no actual move is made\n",
    "    - **Illegal under any real game condition**\n",
    "    - Also includes nonsense like `h2h8` (rook-style moves for pawns)\n",
    "\n",
    "But remember:  \n",
    "> You’re not saying “these are all valid moves”  \n",
    "> **this is the full move *vocabulary***—the possible *labels* in a classification task.\n",
    "> That is, our **vocabulary** list is all the two-tuple pairings of squares, e.g. (`h2g3`), along with other special moves\n",
    "\n",
    "Our total vocabulary size count includes the sum of\n",
    "- 64x64 (each pairing of squares)\n",
    "- 8 (number of columns) x 2 (white-black promotions) x 4 (choices of upgrade) x 3 (capture types) - 16 (edge cases)\n",
    "\n",
    "We should expect our logits vector to have size **4272**. \n",
    "\n",
    "**Benefits of the chosen Vocabulary**\n",
    "\n",
    "1. Simplicity in Output Shape\n",
    "    - 1-to-1 mapping: index ↔ UCI\n",
    "    - You don’t need dynamic output heads or custom decoders\n",
    "    - You can store logits as `torch.tensor([4672])` and just mask out illegal ones at runtime\n",
    "\n",
    "2. Consistency\n",
    "    - Your label space is fixed across:\n",
    "      - Training\n",
    "      - Inference\n",
    "      - Evaluation\n",
    "\n",
    "3. Non-moves Never Get Trained On\n",
    "    - No master ever plays `a1a1`\n",
    "    - So those output indices **never get gradient updates**\n",
    "    - They just sit in the model—harmless dead neurons\n",
    "\n",
    "**Why You Might Remove Redundant Moves**\n",
    "\n",
    "1. Smaller Output Space\n",
    "    - Saves compute on final linear layer and softmax\n",
    "    - Slightly faster training (maybe)\n",
    "\n",
    "2. Model Capacity Allocation\n",
    "    - You force the network to **only ever consider valid move templates**\n",
    "    - Could lead to sharper learning curve\n",
    "\n",
    "But you pay with **more complexity**:\n",
    "- Dynamic move indexing\n",
    "- Pre-mask needs to align with training mask\n",
    "- Harder debugging\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb991bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uci_move_list():\n",
    "    all_moves = set()\n",
    "    for from_sq in chess.SQUARES:\n",
    "        for to_sq in chess.SQUARES:\n",
    "            move = chess.Move(from_sq, to_sq)\n",
    "            all_moves.add(move.uci())\n",
    "            # Add promotions\n",
    "            for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                from_rank = chess.square_rank(from_sq)\n",
    "                to_rank = chess.square_rank(to_sq)\n",
    "                from_file = chess.square_file(from_sq)\n",
    "                to_file = chess.square_file(to_sq)\n",
    "                # Only allow forward promotion (white or black)\n",
    "                if (from_rank , to_rank) in [(6, 7), (1, 0)]:  # white/black promotion ranks\n",
    "                    if abs(from_file - to_file) <= 1:         # straight or diagonal\n",
    "                        promo_move = chess.Move(from_sq, to_sq, promotion=promo)\n",
    "                        all_moves.add(promo_move.uci())\n",
    "    return sorted(all_moves)\n",
    "\n",
    "\n",
    "def save_move_index_map(path=\"data/move_index_map.json\"):\n",
    "    moves = generate_uci_move_list()\n",
    "    uci_to_index = {uci: i for i, uci in enumerate(moves)}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(uci_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5de140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits size =  4272\n"
     ]
    }
   ],
   "source": [
    "move_list = generate_uci_move_list()\n",
    "print('Logits size = ',len(move_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "920d5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# board state gets entries from cuhh\n",
    "PIECE_TO_IDX = {\n",
    "    None: 0,\n",
    "    chess.PAWN: 1,\n",
    "    chess.KNIGHT: 2,\n",
    "    chess.BISHOP: 3,\n",
    "    chess.ROOK: 4,\n",
    "    chess.QUEEN: 5,\n",
    "    chess.KING: 6,\n",
    "}\n",
    "\n",
    "# cuhh make the board go mathematical\n",
    "def encode_board(board: chess.Board):\n",
    "    board_array = np.zeros((8, 8), dtype=np.int64)\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        row = 7 - (square // 8)\n",
    "        col = square % 8\n",
    "\n",
    "        if piece is not None:\n",
    "            base = PIECE_TO_IDX[piece.piece_type]\n",
    "            offset = 0 if piece.color == chess.WHITE else 6\n",
    "            board_array[row][col] = base + offset\n",
    "        else:\n",
    "            board_array[row][col] = 0  # empty\n",
    "\n",
    "    return board_array  # shape: [8,8] of ints in [0,12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77ddd9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  9, 11, 12,  9,  8, 10],\n",
       "       [ 7,  7,  7,  7,  7,  7,  7,  7],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 4,  2,  3,  5,  6,  3,  2,  4]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here it go right here\n",
    "board = chess.Board()\n",
    "encode_board(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "532a3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just a mlp frfr\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32, num_moves=4272):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(13, embedding_dim)  # 13 tokens -> vector\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                # [8,8,32] -> [2048]\n",
    "            nn.Linear(8*8*embedding_dim, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, num_moves)   # Final logits\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # x: [B, 8, 8]\n",
    "        x = self.embed(x)  # [B, 8, 8, D]\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73285571",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PolicyNet(embedding_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d805",
   "metadata": {},
   "source": [
    "### What this do frfr ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479fa28",
   "metadata": {},
   "source": [
    "We want a neural network that takes:\n",
    "- Input: an `8×8` grid of piece tokens (entries are integers from 0 to 12)\n",
    "- Output: a **4672-dimensional logits vector**\n",
    "\n",
    "---\n",
    "\n",
    "### CLASS STRUCTURE: `PolicyNet`\n",
    "\n",
    "```python\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=32, num_moves=4272):\n",
    "```\n",
    "\n",
    "- **`embedding_dim=32`**: each board square will be represented by a **32-dimensional vector**.\n",
    "- **`num_moves=4272`**: the size of **output layer**, corresponding to all possible moves.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        super().__init__()\n",
    "```\n",
    "\n",
    "- Standard for initializing the parent class (`nn.Module`).\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "```python\n",
    "        self.embed = nn.Embedding(13, embedding_dim)\n",
    "```\n",
    "\n",
    "- This layer turns each square’s integer (0–12) into a vector of dimension `[embedding_dim]`.\n",
    "- `[8,8]` board → `[8,8,32]` tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Connected Network (MLP)\n",
    "These are our hidden and output logits layer. \n",
    "```python\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                \n",
    "            nn.Linear(8*8*embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_moves)\n",
    "        )\n",
    "```\n",
    "\n",
    "#### `nn.Flatten()`\n",
    "- Converts `[B, 8, 8, 32]` into `[B, 2048]`, similar to .view\n",
    "- Needed to feed into `Linear` layers\n",
    "\n",
    "#### `nn.Linear(2048, 512)`\n",
    "- Fully connected layer reducing 2048 features to 512 neurons\n",
    "\n",
    "#### `nn.ReLU()`\n",
    "- Non-linear activation to let the model learn more complex patterns\n",
    "\n",
    "#### `nn.Linear(512, num_moves)`\n",
    "- Final layer: predicts logit value for each of the 4672 moves\n",
    "\n",
    "---\n",
    "\n",
    "### `forward` Function\n",
    "\n",
    "```python\n",
    "    def forward(self, x):  # x: [B, 8, 8]\n",
    "        x = self.embed(x)  # [B, 8, 8, D]\n",
    "        x = x.permute(0, 3, 1, 2)  # Optional: [B, D, 8, 8]\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "- **Input**: `x` is a batch of boards, shape `[batch_size, 8, 8]`\n",
    "- **Embedding**: turns each square into a vector: `[B, 8, 8, 32]`\n",
    "- **FC Network**: outputs a `[B, 4272]` tensor of logits\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- network **understands piece identity** through embeddings.\n",
    "- it **flattens** the board to make a prediction using fully connected layers.\n",
    "- The output is a **score for every possible move**, and later **mask illegal ones**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028e809",
   "metadata": {},
   "source": [
    "## DATA SETS\n",
    "\n",
    "Documentation *X*:**INPUT** and *Y*:**TARGET** \n",
    "\n",
    "*X* is a **board state**.\n",
    "\n",
    "> **Board state:**  \n",
    "> - *8×8 matrix*  \n",
    "> - Entries in `[0, 12]`  \n",
    "> - Entries map to **piece type**  \n",
    "\n",
    "*Y* is a list of **move indices**.\n",
    "\n",
    ">**Move Indices (`logits`):**\n",
    "> - vector in $\\mathbb{R}^{4272}$\n",
    "> - entries map to a **move**\n",
    "\n",
    "A reminder of how moves are defined follows. \n",
    "\n",
    ">**Move:**\n",
    "> - pair of grid values (Why?)\n",
    "> - e.g. *d3d2*\n",
    "\n",
    "Recall that the policy net (MLP) outputs a **softmaxed** list of (logits). Training occurs via `cross_entropy`. We read move data from text file in **PGN form**. X lists all the board states of a given game in the order it was played. Y lists all the moves made from a given board state. Let $S_i$ be the $i$ -th board state of the given game, and let $a_i$ be the action (move) made from this state. Then, we may visualize $X$ and $Y$ as the following:\n",
    "\n",
    "$$\\text{Input:}\\ S_1,\\ S_2,\\ ...,\\ S_n$$\n",
    "\n",
    "$$\\text{Output:}\\ a_1,\\ a_2,\\ ...,\\ a_n$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cef91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\samip\\Documents\\quick-maffs\\neural_nets\\makemorechessmoves\\data\\raw_games\\Carlsen.pgn\", \"r\") as file:\n",
    "    game = chess.pgn.read_game(file)\n",
    "    board = game.board()\n",
    "    X, Y = [], []\n",
    "\n",
    "    for move in game.mainline_moves():\n",
    "       board_state = encode_board(board)  # BEFORE the move\n",
    "       try:\n",
    "           move_index = move_list.index(move.uci()) # move is INDEXED here, move list starts from first move\n",
    "           X.append(board_state) # first board state is neutral board, it must be appended to have good beginning game\n",
    "           Y.append(move_index)\n",
    "       except ValueError:\n",
    "           print(f\"Move {move.uci()} not found in move_list. Skipping this move.\")\n",
    "       board.push(move)  # Move AFTER data capture\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "#what happens if we remove winning move (movelist[-1]), will the model still choose victory? >:P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e33b394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2769,  0.3354],\n",
      "        [-0.0834, -0.2623],\n",
      "        [-0.5078,  0.5190],\n",
      "        [ 0.3147,  0.6602],\n",
      "        [ 0.3148,  0.5528],\n",
      "        [-0.5301, -0.8638],\n",
      "        [-0.6249,  0.3751],\n",
      "        [-1.0270, -1.0949],\n",
      "        [-1.7253,  0.1751],\n",
      "        [-1.9656,  0.3177],\n",
      "        [ 0.1604,  0.2982],\n",
      "        [-0.6572, -0.2269],\n",
      "        [-0.2176, -0.1560]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0188,  0.0023,  0.0845,  ...,  0.0628, -0.0132,  0.0742],\n",
      "        [-0.0514, -0.0034,  0.0845,  ...,  0.0316, -0.0465, -0.0582],\n",
      "        [ 0.0131,  0.0725,  0.0181,  ..., -0.0165, -0.0463,  0.0703],\n",
      "        ...,\n",
      "        [-0.0679,  0.0563, -0.0597,  ...,  0.0255, -0.0558,  0.0443],\n",
      "        [ 0.0596,  0.0063, -0.0364,  ..., -0.0352,  0.0788, -0.0103],\n",
      "        [-0.0258, -0.0659,  0.0808,  ..., -0.0689,  0.0096, -0.0495]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0622, -0.0828, -0.0115, -0.0009, -0.0131, -0.0626,  0.0041,  0.0690,\n",
      "        -0.0731,  0.0033, -0.0496,  0.0782, -0.0466,  0.0122, -0.0697,  0.0684,\n",
      "         0.0431, -0.0059, -0.0876,  0.0389,  0.0638, -0.0191, -0.0504,  0.0051,\n",
      "        -0.0863,  0.0764,  0.0306,  0.0679, -0.0072,  0.0273,  0.0840,  0.0684,\n",
      "        -0.0230, -0.0459,  0.0033, -0.0834, -0.0378, -0.0319, -0.0528, -0.0640,\n",
      "        -0.0034,  0.0225,  0.0309,  0.0464,  0.0783,  0.0516, -0.0585, -0.0151,\n",
      "         0.0720, -0.0168,  0.0320,  0.0480,  0.0804, -0.0082,  0.0016, -0.0686,\n",
      "         0.0519, -0.0741,  0.0685, -0.0875,  0.0663,  0.0537,  0.0738, -0.0879,\n",
      "        -0.0063, -0.0123, -0.0479,  0.0700, -0.0145, -0.0835,  0.0194,  0.0304,\n",
      "        -0.0546, -0.0172,  0.0710,  0.0384, -0.0626,  0.0016, -0.0230,  0.0470,\n",
      "         0.0274, -0.0663, -0.0176,  0.0540, -0.0704, -0.0060, -0.0661, -0.0870,\n",
      "        -0.0185, -0.0553, -0.0512,  0.0437, -0.0572, -0.0510,  0.0487, -0.0488,\n",
      "        -0.0416, -0.0508, -0.0638,  0.0869, -0.0042,  0.0325, -0.0542, -0.0823,\n",
      "        -0.0335, -0.0861,  0.0246,  0.0433, -0.0491, -0.0578, -0.0544,  0.0559,\n",
      "         0.0835,  0.0116, -0.0574, -0.0836,  0.0755,  0.0715,  0.0296,  0.0811,\n",
      "        -0.0119, -0.0586, -0.0489,  0.0205,  0.0829,  0.0195, -0.0414,  0.0429,\n",
      "        -0.0381,  0.0047, -0.0352,  0.0845,  0.0830,  0.0120,  0.0152,  0.0336,\n",
      "         0.0072,  0.0110,  0.0624, -0.0631, -0.0869,  0.0110,  0.0461, -0.0015,\n",
      "         0.0679,  0.0792, -0.0335, -0.0178,  0.0160,  0.0462,  0.0534,  0.0546,\n",
      "        -0.0210,  0.0884, -0.0718,  0.0445, -0.0588,  0.0606, -0.0594,  0.0107,\n",
      "         0.0059,  0.0467, -0.0805, -0.0702,  0.0812,  0.0107,  0.0677,  0.0736,\n",
      "        -0.0220, -0.0575, -0.0500, -0.0159,  0.0740, -0.0034, -0.0372, -0.0735,\n",
      "        -0.0491,  0.0428,  0.0122, -0.0129, -0.0807, -0.0824, -0.0598,  0.0313,\n",
      "        -0.0363,  0.0491,  0.0743, -0.0610, -0.0280, -0.0632, -0.0392,  0.0576,\n",
      "        -0.0676, -0.0264,  0.0484,  0.0568,  0.0862, -0.0737, -0.0239,  0.0347,\n",
      "         0.0249, -0.0882,  0.0042,  0.0527,  0.0390,  0.0596, -0.0333,  0.0149,\n",
      "         0.0049,  0.0131,  0.0155, -0.0267, -0.0259, -0.0623,  0.0267,  0.0216,\n",
      "         0.0872, -0.0630,  0.0154, -0.0041, -0.0037, -0.0693,  0.0460, -0.0020,\n",
      "         0.0104,  0.0714,  0.0510, -0.0597,  0.0108,  0.0091,  0.0429,  0.0010,\n",
      "        -0.0056,  0.0637, -0.0247, -0.0164,  0.0861, -0.0032, -0.0242, -0.0616,\n",
      "        -0.0651,  0.0142,  0.0178, -0.0162,  0.0374,  0.0210, -0.0153,  0.0728,\n",
      "         0.0051,  0.0582,  0.0756, -0.0180, -0.0745,  0.0293, -0.0820, -0.0139,\n",
      "         0.0662,  0.0449, -0.0488, -0.0752,  0.0247,  0.0614,  0.0673,  0.0369,\n",
      "        -0.0652, -0.0775, -0.0500, -0.0242,  0.0143,  0.0863, -0.0291, -0.0873,\n",
      "        -0.0452, -0.0438, -0.0387,  0.0343,  0.0380, -0.0789,  0.0043, -0.0464,\n",
      "        -0.0738,  0.0654, -0.0089,  0.0582, -0.0523, -0.0131, -0.0108,  0.0421,\n",
      "         0.0413,  0.0329, -0.0681,  0.0003, -0.0736,  0.0874, -0.0372, -0.0095,\n",
      "        -0.0501,  0.0422, -0.0588,  0.0656, -0.0061,  0.0713,  0.0037, -0.0697,\n",
      "        -0.0711,  0.0481,  0.0435,  0.0714,  0.0662, -0.0187, -0.0550, -0.0621,\n",
      "        -0.0870,  0.0307, -0.0593,  0.0873, -0.0499, -0.0030,  0.0512,  0.0548,\n",
      "        -0.0369, -0.0513,  0.0768, -0.0366,  0.0302,  0.0880, -0.0774, -0.0346,\n",
      "         0.0617, -0.0582, -0.0258, -0.0269, -0.0264, -0.0469, -0.0461,  0.0198,\n",
      "        -0.0190, -0.0810, -0.0353, -0.0660,  0.0217,  0.0209,  0.0274,  0.0052,\n",
      "         0.0401,  0.0848,  0.0433,  0.0462,  0.0794,  0.0803,  0.0663,  0.0850,\n",
      "         0.0046,  0.0322,  0.0799, -0.0185, -0.0076,  0.0748,  0.0422,  0.0760,\n",
      "         0.0035,  0.0050,  0.0104, -0.0360, -0.0612,  0.0256,  0.0614, -0.0194,\n",
      "        -0.0313,  0.0289, -0.0074,  0.0626,  0.0414,  0.0707, -0.0308, -0.0157,\n",
      "         0.0072, -0.0550,  0.0450, -0.0611,  0.0017, -0.0253,  0.0350, -0.0585,\n",
      "         0.0878,  0.0555,  0.0581, -0.0182,  0.0368, -0.0488, -0.0359,  0.0230,\n",
      "         0.0881, -0.0707,  0.0113, -0.0066, -0.0809,  0.0609,  0.0774,  0.0761,\n",
      "         0.0492,  0.0435,  0.0345, -0.0718,  0.0434,  0.0164, -0.0793,  0.0809,\n",
      "        -0.0649,  0.0237, -0.0386, -0.0471,  0.0282,  0.0550, -0.0796, -0.0523,\n",
      "        -0.0636, -0.0140, -0.0315,  0.0836,  0.0336,  0.0615, -0.0699,  0.0393,\n",
      "        -0.0834,  0.0460, -0.0738,  0.0582,  0.0628,  0.0466,  0.0023,  0.0431,\n",
      "        -0.0221, -0.0712,  0.0203,  0.0234,  0.0247, -0.0749, -0.0858, -0.0845,\n",
      "         0.0079, -0.0326,  0.0259,  0.0537, -0.0159,  0.0019, -0.0394, -0.0255,\n",
      "        -0.0171, -0.0600,  0.0411, -0.0528,  0.0453,  0.0099, -0.0097, -0.0176,\n",
      "         0.0601, -0.0698,  0.0642, -0.0349, -0.0480, -0.0002, -0.0178, -0.0507,\n",
      "        -0.0237,  0.0703, -0.0759, -0.0453, -0.0487, -0.0011,  0.0404, -0.0514,\n",
      "        -0.0011, -0.0873, -0.0105,  0.0164, -0.0494, -0.0731, -0.0815, -0.0258,\n",
      "        -0.0129, -0.0279, -0.0695, -0.0311,  0.0566, -0.0255,  0.0631,  0.0317,\n",
      "         0.0256,  0.0749,  0.0040,  0.0861,  0.0220,  0.0836, -0.0845, -0.0305,\n",
      "        -0.0480, -0.0679,  0.0399,  0.0670,  0.0137,  0.0690, -0.0841,  0.0572,\n",
      "        -0.0878,  0.0652, -0.0120,  0.0678, -0.0842,  0.0869,  0.0372, -0.0604],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0146, -0.0231, -0.0393,  ..., -0.0222,  0.0092,  0.0270],\n",
      "        [-0.0229,  0.0025,  0.0211,  ...,  0.0294, -0.0072, -0.0328],\n",
      "        [-0.0408, -0.0080,  0.0080,  ..., -0.0279, -0.0285, -0.0215],\n",
      "        ...,\n",
      "        [-0.0230,  0.0160, -0.0335,  ...,  0.0229, -0.0221,  0.0348],\n",
      "        [-0.0218,  0.0216,  0.0221,  ..., -0.0115, -0.0158,  0.0344],\n",
      "        [ 0.0165,  0.0084, -0.0342,  ...,  0.0255,  0.0387, -0.0147]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0209, -0.0214,  0.0440,  ..., -0.0097, -0.0373, -0.0168],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in mlp.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "lossi = []\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "for i in range(1000): \n",
    "\n",
    "    batch = torch.randint(low=0,high=Y.shape[0], size=(batch_size,))\n",
    "\n",
    "    logits = mlp(X[batch])\n",
    "    loss = F.cross_entropy(logits, Y[batch])\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in mlp.parameters(): \n",
    "        p.data += -lr*p.grad\n",
    "    lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e3f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.0077,  0.7184,  0.0869,  ..., -0.3247,  0.2151,  0.2154]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Probabilities: tensor([[0.0002, 0.0005, 0.0002,  ..., 0.0002, 0.0003, 0.0003]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Convert the board state to a tensor if it's not already\n",
    "board_tensor = torch.tensor(board_state, dtype=torch.long)\n",
    "\n",
    "# Add a batch dimension to the board tensor\n",
    "board_tensor = board_tensor.unsqueeze(0)  # Shape: [1, 8, 8]\n",
    "\n",
    "# Pass the board tensor through the model to get logits\n",
    "output_logits = mlp(board_tensor)\n",
    "\n",
    "# apply softmax to get probabilities\n",
    "output_probs = torch.softmax(output_logits, dim=1)\n",
    "print(\"Logits:\", output_logits)\n",
    "print(\"Probabilities:\", output_probs)\n",
    "\n",
    "# Sample a move index from the probabilities using torch.multinomial\n",
    "sampled_move_index = torch.multinomial(output_probs.squeeze(0), num_samples=1).item()\n",
    "\n",
    "# Retrieve the corresponding UCI move from the move list\n",
    "sampled_move = move_list[sampled_move_index]\n",
    "\n",
    "print(\"Sampled Move Index:\", sampled_move_index)\n",
    "print(\"Sampled Move (UCI):\", sampled_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "260849c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ytr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_steps):\n\u001b[0;32m      9\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m<\u001b[39m(N_steps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[43mYtr\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], size\u001b[38;5;241m=\u001b[39m(batch_size,))\n\u001b[0;32m     11\u001b[0m     emb \u001b[38;5;241m=\u001b[39m C[X[batch]]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# h before the activation\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Ytr' is not defined"
     ]
    }
   ],
   "source": [
    "## specify some training parameters\n",
    "N_steps = 100000\n",
    "print_step = 1000\n",
    "batch_size = 32\n",
    "eps = 1e-5\n",
    "## training the neural net\n",
    "lossi = []\n",
    "for i in range(N_steps):\n",
    "    lr = 0.1 if i<(N_steps/2) else 0.01\n",
    "    batch = torch.randint(low=0, high=Ytr.shape[0], size=(batch_size,))\n",
    "    emb = C[X[batch]]\n",
    "    # h before the activation\n",
    "    hpreact = emb.view(-1,block_size*emd_dim)@W1+b1\n",
    "    hmean = hpreact.mean(0, keepdim=True)\n",
    "    hstd = hpreact.std(0, keepdim=True)\n",
    "    hpreact2 = gamma * (hpreact-hmean)/(hstd+eps) + beta\n",
    "    with torch.no_grad():\n",
    "        mean_runing = 0.999*mean_runing+0.001*hmean\n",
    "        std_runing = 0.999*std_runing+0.001*hstd\n",
    "    # tanh action\n",
    "    h = torch.tanh(hpreact2)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[batch])\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    if i%print_step == 0:\n",
    "        print(f\"step: {i}; loss: {loss.item()}\")\n",
    "    lossi.append(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
